{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предварительно обученное встраивание слов с использованием Glove в модели NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe, — это представление слов в виде векторов в непрерывном векторном пространстве, где угол и направление векторов соответствуют семантическим связям между соответствующими словами. Для этого GloVe строит матрицу совместной встречаемости с использованием пар слов, а затем оптимизирует векторы слов, чтобы минимизировать разницу между взаимной информацией по точкам соответствующих слов и скалярным произведением векторов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как работает GloVe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание матрицы совместного появления слов является фундаментальным компонентом GloVe. Эта матрица обеспечивает количественную меру семантической близости между словами, фиксируя частоту, с которой они появляются вместе в данном контексте. Затем, минимизируя разницу между скалярным произведением векторов и поточечной взаимной информацией соответствующих слов, GloVe оптимизирует векторы слов. GloVe способен создавать плотные векторные представления, которые фиксируют синтаксические и семантические отношения благодаря своей инновационной методологии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создать словарь лексики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Словарь — это набор всех уникальных слов, присутствующих в обучающем наборе данных. Первый набор данных разбивается на слова, затем подсчитывается вся частота каждого слова. Затем слова сортируются в порядке убывания их частот. Слова с высокой частотой помещаются в начало словаря."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм встраивания слов\n",
    "\n",
    "- Предварительная обработка текстовых данных.\n",
    "- Создал словарь.\n",
    "- Пройдитесь по файлу перчаток определенного измерения и сравните каждое слово со всеми словами в словаре,\n",
    "- Если совпадение найдено, скопируйте эквивалентный вектор из перчатки и вставьте в embedding_matrix по соответствующему индексу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# code for Glove word embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    " \n",
    "x = {'text', 'the', 'leader', 'prime',\n",
    "     'natural', 'language'}\n",
    " \n",
    "# create the dict.\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x)\n",
    " \n",
    "# number of unique words in dict.\n",
    "print(\"Number of unique words in dictionary=\", \n",
    "      len(tokenizer.word_index))\n",
    "print(\"Dictionary is = \", tokenizer.word_index)\n",
    " \n",
    "# download glove and unzip it in Notebook.\n",
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!unzip glove*.zip\n",
    " \n",
    "# vocab: 'the': 1, mapping of words with\n",
    "# integers in seq. 1,2,3..\n",
    "# embedding: 1->dense vector\n",
    "def embedding_for_vocab(filepath, word_index,\n",
    "                        embedding_dim):\n",
    "    vocab_size = len(word_index) + 1\n",
    "     \n",
    "    # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix_vocab = np.zeros((vocab_size,\n",
    "                                       embedding_dim))\n",
    " \n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word]\n",
    "                embedding_matrix_vocab[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    " \n",
    "    return embedding_matrix_vocab\n",
    " \n",
    " \n",
    "# matrix for vocab: word_index\n",
    "embedding_dim = 50\n",
    "embedding_matrix_vocab = embedding_for_vocab(\n",
    "    '../glove.6B.50d.txt', tokenizer.word_index,\n",
    "  embedding_dim)\n",
    " \n",
    "print(\"Dense vector for first word is => \",\n",
    "      embedding_matrix_vocab[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выход:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество уникальных слов в словаре = 6 \n",
    "Словарь = {'leader': 1, 'the': 2, 'prime': 3, 'natural': 4, 'language': 5, 'text': 6} \n",
    "Плотный вектор для первого слова => [-0,1567 0,26117 0,78881001 0,65206999 1,20019996 0,35400999 \n",
    "-0,34298 0,31702 -1,15020001 -0,16099 0,15798 -0,53501999 \n",
    "-1,34679997 0,51783001 -0,46441001 -0,19846 0,27474999 -0,26154 \n",
    "  0,25531 0,33388001 -1,04130006 0,52525002 -0,35442999 -0,19137 \n",
    "-0,08964 -2,33139992 0,12433 -0,94405001 -1,02330005 1,35070002 \n",
    "  2,55240011 -0,16897 -1,72899997 0,32548001 -0,30914 -0,63056999 \n",
    "-0,22211 -0,15589 -0,43597999 0,0568 -0,090885 0,75028002 \n",
    "-1,31529999 -0,75358999 0,82898998 0,051397 -1,48049998 -0,11134 \n",
    "  0,27090001 -0,48712999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Определение функции Softmax \n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# Создание класса Word2Vec \n",
    "class word2vec(object):\n",
    "    def __init__(self):\n",
    "        self.N = 10\n",
    "        self.X_train = []\n",
    "        self.y_train = []\n",
    "        self.window_size = 2\n",
    "        self.alpha = 0.001\n",
    "        self.words = []\n",
    "        self.word_index = {}\n",
    " \n",
    "    def initialize(self,V,data):\n",
    "        self.V = V\n",
    "        self.W = np.random.uniform(-0.8, 0.8, (self.V, self.N))\n",
    "        self.W1 = np.random.uniform(-0.8, 0.8, (self.N, self.V))\n",
    "         \n",
    "        self.words = data\n",
    "        for i in range(len(data)):\n",
    "            self.word_index[data[i]] = i\n",
    " \n",
    "    # Прямое распространение \n",
    "    def feed_forward(self,X):\n",
    "        self.h = np.dot(self.W.T,X).reshape(self.N,1)\n",
    "        self.u = np.dot(self.W1.T,self.h)\n",
    "        #print(self.u)\n",
    "        self.y = softmax(self.u)  \n",
    "        return self.y\n",
    "\n",
    "    # Обратное распространение ошибки    \n",
    "    def backpropagate(self,x,t):\n",
    "        e = self.y - np.asarray(t).reshape(self.V,1)\n",
    "        # e.shape is V x 1\n",
    "        dLdW1 = np.dot(self.h,e.T)\n",
    "        X = np.array(x).reshape(self.V,1)\n",
    "        dLdW = np.dot(X, np.dot(self.W1,e).T)\n",
    "        self.W1 = self.W1 - self.alpha*dLdW1\n",
    "        self.W = self.W - self.alpha*dLdW\n",
    "\n",
    "    # Обучение модели  \n",
    "    def train(self,epochs):\n",
    "        for x in range(1,epochs):        \n",
    "            self.loss = 0\n",
    "            for j in range(len(self.X_train)):\n",
    "                self.feed_forward(self.X_train[j])\n",
    "                self.backpropagate(self.X_train[j],self.y_train[j])\n",
    "                C = 0\n",
    "                for m in range(self.V):\n",
    "                    if(self.y_train[j][m]):\n",
    "                        self.loss += -1*self.u[m][0]\n",
    "                        C += 1\n",
    "                self.loss += C*np.log(np.sum(np.exp(self.u)))\n",
    "            print(\"epoch \",x, \" loss = \",self.loss)\n",
    "            self.alpha *= 1/( (1+self.alpha*x) )\n",
    "\n",
    "    # Прогнозирование        \n",
    "    def predict(self,word,number_of_predictions):\n",
    "        if word in self.words:\n",
    "            index = self.word_index[word]\n",
    "            X = [0 for i in range(self.V)]\n",
    "            X[index] = 1\n",
    "            prediction = self.feed_forward(X)\n",
    "            output = {}\n",
    "            for i in range(self.V):\n",
    "                output[prediction[i][0]] = i\n",
    "             \n",
    "            top_context_words = []\n",
    "            for k in sorted(output,reverse=True):\n",
    "                top_context_words.append(self.words[output[k]])\n",
    "                if(len(top_context_words)>=number_of_predictions):\n",
    "                    break\n",
    "     \n",
    "            return top_context_words\n",
    "        else:\n",
    "            print(\"Word not found in dictionary\")\n",
    "          \n",
    "\n",
    "# Предварительная обработка корпуса (очищает и подготавливает текстовые данные, удаляя стоп-слова и знаки препинания, а также преобразуя слова в нижний регистр)\n",
    "def preprocessing(corpus):\n",
    "    stop_words = set(stopwords.words('english'))    \n",
    "    training_data = []\n",
    "    sentences = corpus.split(\".\")\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = sentences[i].strip()\n",
    "        sentence = sentences[i].split()\n",
    "        x = [word.strip(string.punctuation) for word in sentence\n",
    "                                     if word not in stop_words]\n",
    "        x = [word.lower() for word in x]\n",
    "        training_data.append(x)\n",
    "    return training_data\n",
    "     \n",
    " \n",
    "def prepare_data_for_training(sentences,w2v):\n",
    "    data = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word not in data:\n",
    "                data[word] = 1\n",
    "            else:\n",
    "                data[word] += 1\n",
    "    V = len(data)\n",
    "    data = sorted(list(data.keys()))\n",
    "    vocab = {}\n",
    "    for i in range(len(data)):\n",
    "        vocab[data[i]] = i\n",
    "     \n",
    "    #for i in range(len(words)):\n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            center_word = [0 for x in range(V)]\n",
    "            center_word[vocab[sentence[i]]] = 1\n",
    "            context = [0 for x in range(V)]\n",
    "            \n",
    "            for j in range(i-w2v.window_size,i+w2v.window_size):\n",
    "                if i!=j and j>=0 and j<len(sentence):\n",
    "                    context[vocab[sentence[j]]] += 1\n",
    "                    w2v.X_train.append(center_word)\n",
    "                    w2v.y_train.append(context)\n",
    "    w2v.initialize(V,data)\n",
    " \n",
    "    return w2v.X_train,w2v.y_train   \n",
    "\n",
    "# Запуск обучения и прогнозирования\n",
    "corpus = \"\"\n",
    "corpus += \"The earth revolves around the sun. The moon revolves around the earth\"\n",
    "epochs = 1000\n",
    "\n",
    "training_data = preprocessing(corpus)\n",
    "w2v = word2vec()\n",
    "\n",
    "prepare_data_for_training(training_data,w2v)\n",
    "w2v.train(epochs) \n",
    "\n",
    "print(w2v.predict(\"around\",3))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение функции Softmax\n",
    "\n",
    "Функция softmax используется для преобразования сырых оценок (логитов) в вероятности. Она обычно используется в выходном слое нейронной сети для задач классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание класса Word2Vec\n",
    "\n",
    "Мы определяем word2vecкласс, который будет содержать методы для инициализации весов, выполнения прямого распространения, обратного распространения, обучения и прогнозирования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прямое распространение\n",
    "\n",
    "Метод прямого распространения вычисляет активации скрытого слоя и вероятности выходного слоя с использованием функции softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратное распространение ошибки\n",
    "\n",
    "Метод обратного распространения корректирует веса на основе ошибки между прогнозируемым выходом и фактическими словами контекста. Он вычисляет градиенты и обновляет матрицы весов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение модели\n",
    "\n",
    "Метод train итерирует данные обучения для указанного количества эпох. В каждой эпохе он выполняет прямое распространение, обратное распространение и вычисляет потери."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прогнозирование\n",
    "\n",
    "Метод прогнозирования берет слово и возвращает верхние контекстные слова на основе обученной модели. Он использует метод прямого распространения для получения вероятностей и сортирует их для поиска наиболее вероятных контекстных слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предварительная обработка корпуса\n",
    "\n",
    "Функция предварительной обработки очищает и подготавливает текстовые данные, удаляя стоп-слова и знаки препинания, а также преобразуя слова в нижний регистр."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка данных для обучения\n",
    "\n",
    "Функция prepare_data_for_training создает обучающие данные путем генерации векторов прямого кодирования для центрального и контекстного слов на основе размера окна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запуск обучения и прогнозирования\n",
    "\n",
    "Наконец, мы запускаем этапы предварительной обработки, обучения и прогнозирования. Мы определяем корпус, проводим его предварительную обработку, готовим данные для обучения, обучаем модель и делаем прогнозы."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
