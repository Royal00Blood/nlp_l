{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель «мешок слов» (BoW) в НЛП"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте возьмем этот пример абзаца для нашей задачи:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фасоль. Я пытался объяснить кому-то, когда мы летели, что это кукуруза. Это фасоль. И они были очень впечатлены моими сельскохозяйственными познаниями. Пожалуйста, еще раз поаплодируйте Амори за это выдающееся представление. У меня здесь сегодня много хороших друзей, включая того, с кем я служил, кто является одним из лучших сенаторов в стране, и нам повезло, что он, ваш сенатор Дик Дурбин, здесь. Кстати, я также заметил бывшего губернатора Эдгара, которого я давно не видел, и каким-то образом он не постарел, а я постарел. И здорово видеть вас, губернатор. Я хочу поблагодарить президента Киллина и всех в системе университета Иллинойса за то, что вы сделали возможным мое присутствие здесь сегодня. И я глубоко польщен премией Пола Дугласа, которая мне вручается. Он тот, кто проложил путь к столь выдающейся государственной службе здесь, в Иллинойсе. Теперь я хочу начать с обращения к слону в комнате. Я знаю, что люди до сих пор задаются вопросом, почему я не выступил на церемонии вручения дипломов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шаг №1: Сначала мы выполним предварительную обработку данных, чтобы:\n",
    "\n",
    "- Преобразовать текст в нижний регистр.\n",
    "- Удалите все несловесные символы.\n",
    "- Удалите все знаки препинания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_rus.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "import re \n",
    "import numpy as np \n",
    "\n",
    "nltk.download('all')\n",
    "\n",
    "# execute the text here as : \n",
    "text = \"Фасоль. Я пытался объяснить кому-то, когда мы летели, что это кукуруза. Это фасоль. И они были очень впечатлены моими сельскохозяйственными познаниями. Пожалуйста, еще раз поаплодируйте Амори за это выдающееся представление. У меня здесь сегодня много хороших друзей, включая того, с кем я служил, кто является одним из лучших сенаторов в стране, и нам повезло, что он, ваш сенатор Дик Дурбин, здесь. Кстати, я также заметил бывшего губернатора Эдгара, которого я давно не видел, и каким-то образом он не постарел, а я постарел. И здорово видеть вас, губернатор. Я хочу поблагодарить президента Киллина и всех в системе университета Иллинойса за то, что вы сделали возможным мое присутствие здесь сегодня. И я глубоко польщен премией Пола Дугласа, которая мне вручается. Он тот, кто проложил путь к столь выдающейся государственной службе здесь, в Иллинойсе. Теперь я хочу начать с обращения к слону в комнате. Я знаю, что люди до сих пор задаются вопросом, почему я не выступил на церемонии вручения дипломов.\" \n",
    "dataset = nltk.sent_tokenize(text, language='russian') \n",
    "for i in range(len(dataset)): \n",
    "    dataset[i] = dataset[i].lower() \n",
    "    dataset[i] = re.sub(r'\\W', ' ', dataset[i]) \n",
    "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шаг №2: Получение наиболее часто встречающихся слов в тексте.\n",
    "\n",
    "Для создания нашей модели мы применим следующие шаги.\n",
    "\n",
    "- Мы объявляем словарь хранителем наших слов.\n",
    "- Далее мы разбиваем каждое предложение на слова.\n",
    "- Теперь для каждого слова в предложении мы проверяем, существует ли это слово в нашем словаре.\n",
    "- Если это так, то мы увеличиваем его счетчик на 1. Если нет, то мы добавляем его в наш словарь и устанавливаем его счетчик равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model \n",
    "word2count = {} \n",
    "for data in dataset: \n",
    "\twords = nltk.word_tokenize(data) \n",
    "\tfor word in words: \n",
    "\t\tif word not in word2count.keys(): \n",
    "\t\t\tword2count[word] = 1\n",
    "\t\telse: \n",
    "\t\t\tword2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашей модели всего 118 слов. Однако при обработке больших текстов количество слов может достигать миллионов. Нам не нужно использовать все эти слова. Поэтому мы выбираем определенное количество наиболее часто используемых слов. Для реализации этого мы используем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq \n",
    "freq_words = heapq.nlargest(100, word2count, key=word2count.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где 100 обозначает количество слов, которое мы хотим. Если наш текст большой, мы вводим большее число."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шаг №3: Создание модели «мешка слов»\n",
    "\n",
    "На этом шаге мы строим вектор, который скажет нам, является ли слово в каждом предложении частым словом или нет. Если слово в предложении является частым словом, мы устанавливаем его как 1, в противном случае мы устанавливаем его как 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] \n",
    "for data in dataset: \n",
    "\tvector = [] \n",
    "\tfor word in freq_words: \n",
    "\t\tif word in nltk.word_tokenize(data): \n",
    "\t\t\tvector.append(1) \n",
    "\t\telse: \n",
    "\t\t\tvector.append(0) \n",
    "\tX.append(vector) \n",
    "X = np.asarray(X) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
